---
layout: post
title: "Neural network mdoel deployment"
author: "Timothy Shan"
tags: dl
---

## About 

This post will cover the experimentation for deploying the pre-trained neural network models for on-board computers, such as NUC11. I have experimented with the following framework/toolbox 

- [onnx](https://onnx.ai/)
- [MMDeploy](https://mmdeploy.readthedocs.io/en/latest/)
- [NVIDIA TensorRT](https://developer.nvidia.com/tensorrt)
- [TVM](https://tvm.apache.org/)

## ONNX 

Previously, our segmentation model was trained using tensorflow, based on [DeepLabv3](https://github.com/tensorflow/models/tree/master/research/deeplab). For deployment, we used `uff` format before. However, there are unsupported operations to convert uff to TensorRT, and need to compile TensorRT from source. 

To circumvent this issue with uff format, we can also convert the models trained from tensorflow/pytorch to [onnx](https://onnx.ai/) format, which defines a set of common operations in a common file format.  The saved model in frozen graph format can be converted to onnx via 
```bash 
python -m tf2onnx.convert --graphdef frozen_inference_graph_segmentation.pb --output frozen_inference_graph_segmentation.onnx \
    --opset 11 \
    --inputs model/image:0[1,320,576,3] \
    --outputs model/semantic_segmentation/semantic_segmentation/ArgMax:0  
```

To do inference using onnx, use the script below 
```python
import tensorrt as trt
import numpy as np
import os

import pycuda.driver as cuda
import pycuda.autoinit
import cv2
import torch
from torchvision import transforms
import numpy as np
import onnx
import onnxruntime
from matplotlib import pyplot as plt

def normalise_input(im_float32: np.ndarray) -> np.ndarray:

    mean = np.mean(im_float32, keepdims=True)

    rsd = 1.0 / np.sqrt(np.mean(np.square(im_float32), keepdims=True) - np.square(mean))

    rsd = np.minimum(rsd, 3 * np.prod(im_float32.shape))

    im_float32 = (im_float32 - mean) * rsd

    im_float32 = np.clip(0.2 * im_float32 + 0.5, 0.0, 1.0)
    im_float32 = im_float32.astype(np.float32)

    return im_float32


if __name__ == "__main__":

    batch_size = 1
    filename = "frozen_inference_graph_segmentation.onnx"
    onnx_path = os.path.join("/home/sean/workspace/trt_models", filename)

    onnx_model = onnx.load(onnx_path)
    onnx.checker.check_model(onnx_model)
    ort_session = onnxruntime.InferenceSession(onnx_path)

    img_path = os.path.join("/home/sean/workspace/tensorrt_ws/data", "frame_00207_1566804935.650853634.png")
    input_img = cv2.imread(img_path) 
    input_img = cv2.resize(input_img, (576, 320), interpolation = cv2.INTER_AREA)
    # cv2.imshow("image", input_img)
    # cv2.waitKey(0)
    data = normalise_input(input_img.astype(np.float32) / 255.0)
    data = np.expand_dims(data, axis=0)

    ort_inputs = {ort_session.get_inputs()[0].name: data}
    ort_outs = ort_session.run(None, ort_inputs)
    preds = ort_outs[0]
    onnx_image = torch.from_numpy(preds)
    onnx_image = onnx_image.squeeze(0)
    onnx_image = onnx_image.detach().cpu().numpy()
    onnx_image = onnx_image.astype(np.uint8)

    num_classes = 7

    fig, axs = plt.subplots(1, 2, figsize=(16, 16))

    images = []

    axs[0].set_title("Image")
    axs[1].set_title("Prediction")

    images.append(axs[0].imshow(input_img.astype(int)))
    images.append(axs[1].imshow(onnx_image, cmap=plt.get_cmap('nipy_spectral'), vmin=0, vmax=num_classes))

    seg_classes = [
          "general_obstacle",
          "footpath_or_floor",
          "tactile_grid",
          "grass_or_dirt",
          "road",
          "traffic_light_crossing",
          "zebra_crossing",
    ]

    cbar = fig.colorbar(images[1], ax=axs, orientation='horizontal', ticks=[x for x in range(num_classes)], fraction=.1)
    cbar.ax.set_xticklabels(list(seg_classes), rotation=55)

    plt.show()
    plt.pause(0)
```

The converted onnx model works fine in python, as can be seen below 
![img](../assets/posts/22-06/seg-onnx.png)

## TensorRT

## Segmentation 

The onnx model can be converted to TensorRT following [this guide](https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html#convert-model), or via the script below 
```python
import os
import time
import torch
import torchvision
import numpy as np
import tensorrt as trt

os.environ["CUDA_VISIBLE_DEVICES"] = '0'

TRT_LOGGER = trt.Logger(trt.Logger.VERBOSE)


def build_engine(onnx_file_path, engine_file_path, max_batch_size=1, save_engine=False):
    """
    Args:
      max_batch_size: 
      save_engine:    
    return:
      ICudaEngine
    """
    EXPLICIT_BATCH = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
    with trt.Builder(TRT_LOGGER) as builder, builder.create_network(EXPLICIT_BATCH) as network, builder.create_builder_config() as config, trt.OnnxParser(network, TRT_LOGGER) as parser: 
        config = builder.create_builder_config()
        config.max_workspace_size = 2 << 30
        builder.max_batch_size = max_batch_size        

        if not os.path.exists(onnx_file_path):
            quit("ONNX file {} not found!".format(onnx_file_path))
        print('loading onnx file from path {} ...'.format(onnx_file_path))
        with open(onnx_file_path, 'rb') as model:
            print("Begining onnx file parsing")
            if not parser.parse(model.read()):         
                print('ERROR: Failed to parse the ONNX file.')
                for error in range(parser.num_errors):
                    print(parser.get_error(error))     
                return None

        last_layer = network.get_layer(network.num_layers - 1)
        # Check if last layer recognizes it's output
        if not last_layer.get_output(0):
            # If not, then mark the output using TensorRT API
            network.mark_output(last_layer.get_output(0))
        print("Completed parsing of onnx file")

        print("Building an engine from file{}' this may take a while...".format(onnx_file_path))
        engine=builder.build_engine(network, config)  
        print("Completed creating Engine")
        if save_engine:
            with open(engine_file_path, 'wb') as f:
                f.write(engine.serialize())
        return engine


if __name__ == '__main__':
    root_dir = "/home/sean/workspace/trt_models/"

    onnx_file_path = root_dir + "frozen_inference_graph_segmentation.onnx"
    # onnx_file_path = root_dir + "frozen_inference_graph_detection.onnx"

    trt_engine_path = root_dir + "frozen_inference_graph_segmentation.trt8.bin"
    # trt_engine_path = root_dir + "frozen_inference_graph_detection.trt8.bin"

    max_batch_size = 1
    engine = build_engine(onnx_file_path, trt_engine_path, max_batch_size, save_engine=True)
```

Then we can do inference using 
```python 
import tensorrt as trt
import numpy as np
import os

import pycuda.driver as cuda
import pycuda.autoinit
import cv2
import torch
from torchvision import transforms
import numpy as np


def normalise_input(im_float32: np.ndarray) -> np.ndarray:

    mean = np.mean(im_float32, keepdims=True)

    rsd = 1.0 / np.sqrt(np.mean(np.square(im_float32), keepdims=True) - np.square(mean))

    rsd = np.minimum(rsd, 3 * np.prod(im_float32.shape))

    im_float32 = (im_float32 - mean) * rsd

    im_float32 = np.clip(0.2 * im_float32 + 0.5, 0.0, 1.0)

    return im_float32


class HostDeviceMem(object):
    def __init__(self, host_mem, device_mem):
        self.host = host_mem
        self.device = device_mem

    def __str__(self):
        return "Host:\n" + str(self.host) + "\nDevice:\n" + str(self.device)

    def __repr__(self):
        return self.__str__()

class TrtModel:
    
    def __init__(self,engine_path,max_batch_size=1,dtype=np.float32):
        
        self.engine_path = engine_path
        self.dtype = dtype
        self.logger = trt.Logger(trt.Logger.WARNING)
        self.runtime = trt.Runtime(self.logger)
        self.engine = self.load_engine(self.runtime, self.engine_path)

        self.max_batch_size = max_batch_size
        self.inputs, self.outputs, self.bindings, self.stream = self.allocate_buffers()
        self.context = self.engine.create_execution_context()

                
    @staticmethod
    def load_engine(trt_runtime, engine_path):
        trt.init_libnvinfer_plugins(None, "")             
        with open(engine_path, 'rb') as f:
            engine_data = f.read()
        engine = trt_runtime.deserialize_cuda_engine(engine_data)
        return engine
    
    def allocate_buffers(self):
        
        inputs = []
        outputs = []
        bindings = []
        stream = cuda.Stream()
        
        for binding in self.engine:
            size = trt.volume(self.engine.get_binding_shape(binding)) * self.max_batch_size
            host_mem = cuda.pagelocked_empty(size, self.dtype)
            device_mem = cuda.mem_alloc(host_mem.nbytes)
            
            bindings.append(int(device_mem))

            if self.engine.binding_is_input(binding):
                inputs.append(HostDeviceMem(host_mem, device_mem))
            else:
                outputs.append(HostDeviceMem(host_mem, device_mem))
        
        return inputs, outputs, bindings, stream
       
            
    def __call__(self,x:np.ndarray,batch_size=2):
        
        x = x.astype(self.dtype)
        
        np.copyto(self.inputs[0].host,x.ravel())
        
        for inp in self.inputs:
            cuda.memcpy_htod_async(inp.device, inp.host, self.stream)
        
        self.context.execute_async(batch_size=batch_size, bindings=self.bindings, stream_handle=self.stream.handle)
        for out in self.outputs:
            cuda.memcpy_dtoh_async(out.host, out.device, self.stream) 
            
        
        self.stream.synchronize()
        return [out.host.reshape(batch_size,-1) for out in self.outputs
        
if __name__ == "__main__":
 
    batch_size = 1
    filename = "frozen_inference_graph_segmentation.trt"
    # filename = "frozen_inference_graph_detection.trt8.bin"
    trt_engine_path = os.path.join("/home/sean/workspace/trt_models", filename)
    model = TrtModel(trt_engine_path)
    shape = model.engine.get_binding_shape(0)

    img_path = os.path.join("/home/sean/workspace/tensorrt_ws/data", "frame_00207_1566804935.650853634.png")
    input_img = cv2.imread(img_path) 
    input_img = cv2.resize(input_img, (576, 320), interpolation = cv2.INTER_AREA)
    # cv2.imshow("image", input_img)
    # cv2.waitKey(0)
    data = normalise_input(input_img.astype(np.float32) / 255.0)
    data = np.expand_dims(data, axis=0)
    # data = np.random.randint(0,255,(batch_size,*shape[1:]))/255

    # result = model(data, batch_size)
    # print(result.shape)

    engine = model.engine 
    # get sizes of input and output and allocate memory required for input data and for output data
    for binding in engine:
        if engine.binding_is_input(binding):  # we expect only one input
            input_shape = engine.get_binding_shape(binding)
            input_size = trt.volume(input_shape) * engine.max_batch_size * np.dtype(np.float32).itemsize  # in bytes
            device_input = cuda.mem_alloc(input_size)
        else:  # and one output
            output_shape = engine.get_binding_shape(binding)
            # create page-locked memory buffers (i.e. won't be swapped to disk)
            host_output = cuda.pagelocked_empty(trt.volume(output_shape) * engine.max_batch_size, dtype=np.float32)
            device_output = cuda.mem_alloc(host_output.nbytes)

    host_input = np.array(data, dtype=np.float32, order='C')
    cuda.memcpy_htod(device_input, host_input)

    # run inference
    context = engine.create_execution_context()
    context.execute(bindings=[int(device_input), int(device_output)])
    cuda.memcpy_dtoh(host_output, device_output)

    # postprocess results
    output_data = torch.Tensor(host_output).reshape(engine.max_batch_size, output_shape[0])
    print(output_data)
```

However, the output for the segmentation mask contains all zero, maybe due to precision loss during conversion, or unsupported layer, etc. 

Furthermore, we are using Rust for developing the perception module, and TensorRT does not natively support Rust. We need to use [cxx](https://cxx.rs/) for the FFI bridge with TensorRT's C++ APIs. 

TODO:
- [ ] need to debug the output layer by layer and check what's the cause for zero-value output

## Object detection 

I also tried to convert the object detection model trained based on RetinaNet. At first I tried 
```
python -m tf2onnx.convert --graphdef frozen_inference_graph_detection.pb --output frozen_inference_graph_detection.onnx \
    --opset 11 \
    --inputs model/image:0 \
    --outputs model/object_detection/box_predictor/filtered_positions:0,model/object_detection/box_predictor/filtered_scores:0,model/object_detection/box_predictor/filtered_labels:0
```

However, the masking step before NMS is not supported by TensorRT, more details can be found in [this blog](https://paulbridger.com/posts/tensorrt-object-detection-quantized/). 

I changed the output layer and only keep the graph before the NMS, which works 
```
python -m tf2onnx.convert --graphdef frozen_inference_graph_detection.pb --output frozen_inference_graph_detection.onnx \
    --opset 11 \
    --inputs model/image:0 \
    --outputs model/object_detection/box_predictor/regression:0,model/object_detection/box_predictor/classification:0
```

## MMDeploy 

[MMDetection](https://github.com/open-mmlab/mmdetection) is a toolbox based on pytorch that supports multiple object detection frameworks, such as Faster RCNN, Mask RCNN, RetinaNet, etc. 
MMDeploy facilitates the deployment by supported multiple backends. Using MMDeploy, the models trained with MMDetection can be exported to onnx, TensorRT, etc. 

I tried to port the models to onnx based on [this guide](https://github.com/open-mmlab/mmdetection/blob/master/docs/en/tutorials/pytorch2onnx.md), but the TensorRT segmentation output is still zero. 

Another option is to directly use the C++ SDK, but it says `Caution: The C++ API is highly volatile and not recommended at the moment` [here](https://github.com/open-mmlab/mmdeploy/blob/master/docs/en/get_started.md#integrate-mmdeploy-sdk). 

## TVM

TVM is another backend similar to TensorRT. It has a [Rust wrapper](https://github.com/apache/tvm/tree/main/rust), which supports the [graph executor](https://tvm.apache.org/docs/reference/api/python/graph_executor.html). However, it does not support the newly developed [virtual machine](https://tvm.apache.org/docs//arch/virtual_machine.html). 
TVM also supports TensorRT as a backend as described [here](https://tvm.apache.org/docs/how_to/deploy/tensorrt.html), which could be used to work with [NVDLA](http://nvdla.org/). 

### Segmentation 

Since the onnx model converted from the frozen graph for segmentation works, the first thing I tried with tvm was to create a relay module, save it, then load it in Rust. However, I encountered error when using cuda (no issues when using Python, only had this error when using in Rust)
```
signal: 11, SIGSEGV: invalid memory reference
```

Then I switched to cpu version, and found that although the Rust version can do inference, the results were totally wrong
![img](../assets/posts/22-06/seg-rust-wrong.png)

I have experimented with several pre-trained models, and still cannot get correct output. After several trials, the one I found working is the UNet from [here](https://github.com/gasparian/multiclass-semantic-segmentation). The output from python is 
![img](../assets/posts/22-06/seg-python.png)

The output from Rust is 
![img](../assets/posts/22-06/seg-demo.gif)

The output from Rust is worse compared with Python version, due to quantization. The conversion script is based on [this blog](https://spell.ml/blog/optimizing-pytorch-models-using-tvm-YI7pvREAACMAwYYz), which states that `tvm.relay.frontend.from_pytorch` only takes a quantized model as input. 
Efficiency-wise, segmentation takes about 356.32 ms per image, which is 2.81 Hz. I also tried tuning the tvm model `tvm_optimized_module = tune(mod, params, X_ex)` from [here](https://github.com/spellml/examples/blob/master/external/tvm/scripts/test_mobilenet.py) but could not increase the efficiency and performance. 

### Object detection 

It's much harder to make object detection work compared to segmentation. I tried all available pre-trained models from Pytorch/tensorflow/mxnet/mmdeploy, but could not get the correct output from Rust. 

- Convert models such as `SSD`, `RetinaNet` to onnx via mmdeploy then convert to tvm, error is `TVM doesn’t support dynamic input shapes` as from [here](https://discuss.tvm.apache.org/t/relay-does-relay-frontend-support-dynamic-input-shapes/1604/4)
    - tried to set `--dynamic-export` to `false` in [this](https://mmdetection.readthedocs.io/en/latest/tutorials/pytorch2onnx.html#usage) but still same error 
    - also tried [ONNX Simplifier](https://github.com/daquexian/onnx-simplifier) but same error 
    - could not identify where dynamic shape is involved, I suspect it's related to NMS 
    - dynamic shape is supported with virtual machine, but VM is not supported in the Rust wrapper 

- Since we need to use graph executor instead of VM in Rust, I also tried mxnet, the support from tvm is the best for mxnet and there's no issue converting the models. However, even though the detection in Python is correct, the results are all wrong in Rust

- For tensorflow, tvm support is avaiable, but need to get graph definition in tf first before converting to tvm graph, also some operations are not supported, e.g. 
    - `TVMError: if is not supported` same as in [this](https://discuss.tvm.apache.org/t/relay-tvmerror-if-is-not-supported/6599)
    
- tvm's support for pytorch is not as good as mxnet/tensorflow, there are many operations not supported, but pytorch provides an easy way for quantization and can directly export to tvm graph
    - `The following operators are not implemented: [‘aten::triu’, ‘prim::PythonOp’, ‘aten::copy_’]` same as in [this](https://discuss.tvm.apache.org/t/the-following-operators-are-not-implemented-aten-triu-prim-pythonop-aten-copy/12851)

After some experimentation, I found [DETR](https://github.com/facebookresearch/detr) works in Rust. Here's the script to convert the Pytorch model to tvm:  
```python
# Model code adpated from:
# https://github.com/spellml/mobilenet-cifar10/blob/master/servers/eval_quantized_t4.py
import math
import os
import time
import numpy as np
import cv2
from matplotlib import pyplot as plt

import torch
from torch import optim
import torch.nn as nn
import torchvision
from torch.utils.data import DataLoader

from detr import DETRdemo
from tvm_funcs import get_tvm_model, tune, time_it

import torch
from torch import nn
from torchvision.models import resnet50
import torchvision.transforms as T
torch.set_grad_enabled(False)

class TraceWrapper(torch.nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model

    def forward(self, img):
        outputs = self.model(img)
        labels = torch.argmax(outputs['pred_logits'], dim=2)
        # print(f"labels {labels}")
        # keep only predictions with 0.7+ confidence
        probas = outputs['pred_logits'].softmax(-1)[0, :, :-1].max(-1).values
        return labels, outputs['pred_boxes'], probas

def get_model():
    detr = DETRdemo(num_classes=91)
    state_dict = torch.hub.load_state_dict_from_url(
        url='https://dl.fbaipublicfiles.com/detr/detr_demo-da2a99e9.pth',
        map_location='cpu', check_hash=True)
    detr.load_state_dict(state_dict)
    detr.eval()
    detr = TraceWrapper(detr)
    return detr

def preprocess(image):
    MEAN = np.array([0.485, 0.456, 0.406])
    STD = np.array([0.229, 0.224, 0.225])

    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0
    image -= MEAN
    image /= STD
    image = cv2.resize(image, (848, 480))
    image = image.transpose(2, 0, 1)
    image = np.expand_dims(image, 0)
    return image

if __name__ == "__main__":
    image = cv2.imread("/home/sean/workspace/ros2_tvm/data/2011_09_26-0056-0000000081-003157.png")
    image = preprocess(image)
    input_img = torch.from_numpy(image)

    model = get_model()
    output = model(input_img)
    # print(output)

    print(f"Converting the model (post-training)...")
    start_time = time.time()
    quantized = torch.quantization.convert(model)
    print(f"Quantization done in {str(time.time() - start_time)} seconds.")
    torch.save(quantized.state_dict(), "/home/sean/workspace/ros2_tvm/model/quantized.pth")

    # tvm part
    mod, params, module, lib = get_tvm_model(quantized, input_img)

    # save and load the code and lib file.
    dir = "/home/sean/workspace/ros2_tvm/model/"
    path_lib = os.path.join(dir, "detection_lib.so")
    lib.export_library(path_lib)
```

And the saved model can be called in Rust:
```rust
pub fn detect_objects(img: &cv2::core::Mat) -> cv2::core::Mat {
    let mut img_display = img.clone();
    let image_arr: nd::ArrayView3<u8> = img.try_as_array().unwrap();
    let arr = preprocess(image_arr);

    let now = Instant::now();
    let dev = trt::Device::cpu(0);
    let input =
        trt::NDArray::from_rust_ndarray(&arr, dev, trt::DataType::float(32, 1)).unwrap();
    // load the built module
    let lib = trt::Module::load(&Path::new("./model/detection_lib.so")).unwrap();
    let mut graph_rt = trt::graph_rt::GraphRt::from_module(lib, dev).unwrap();
    graph_rt.set_input("input0", input).unwrap();
    graph_rt.run().unwrap();
    let elapsed = now.elapsed();
    println!("Time elapsed for object detection: {:.2?}", elapsed);
    println!("Frequency for object detection: {:.2?}", 1.0/(elapsed.as_nanos() as f64 / 1e9));

    // prepare to get the output
    let labels_nd = graph_rt.get_output(0).unwrap();
    let bboxes_nd = graph_rt.get_output(1).unwrap();
    let probs_nd = graph_rt.get_output(2).unwrap();

    let labels: Vec<i32> = labels_nd.to_vec::<i32>().unwrap();
    // println!("labels {:?}", labels);
    let bboxes_flat: Vec<f32> = bboxes_nd.to_vec::<f32>().unwrap();
    // println!("bboxes_flat shape {}", bboxes_flat.len());
    let bboxes: Vec<Vec<f32>> = bboxes_flat.chunks(4).map(|x| x.to_vec()).collect();
    let probs: Vec<f32> = probs_nd.to_vec::<f32>().unwrap();

    for (i, bbox) in bboxes.iter().enumerate() {
        if probs[i] < 0.7 {
            continue;
        }
        // println!("probs {}", probs[i]);
        // println!("label {}", labels[i]);
        // println!("bbox {:?}", bbox);
        let rect = bbox2rect(bbox);
        let color = cv2::core::Scalar::new(255f64, 0f64, 0f64, -1f64);
        plot_rect_cv(&mut img_display, rect, color);
    }

    return img_display;
}
```

For efficiency, object detection takes about 478.95 ms per image, which is 2.06 Hz. The output is below:
![img](../assets/posts/22-06/det-demo.gif) 

The repo for this blog is [here](https://github.com/shanmo/ros2_tvm). 
